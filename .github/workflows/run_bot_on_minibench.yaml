name: Forecast on MiniBench tournament

on:
  workflow_dispatch:
    inputs:
      tournament_slug:
        description: 'MiniBench tournament slug (e.g., minibench)'
        required: false
        type: string
      tournament_id:
        description: 'MiniBench tournament ID (overrides repo var)'
        required: false
        type: string
      scheduling_frequency_minutes:
        description: 'Run frequency in minutes'
        required: false
        default: '60'
        type: string
  schedule:
    # Check every 15 minutes by default (MiniBench can open short-lived questions)
    - cron: '*/15 * * * *'

concurrency:
  group: minibench-forecasting
  cancel-in-progress: true

jobs:
  minibench_forecast:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    env:
      METACULUS_TOKEN: ${{ secrets.METACULUS_TOKEN }}
      OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      ASKNEWS_CLIENT_ID: ${{ secrets.ASKNEWS_CLIENT_ID }}
      ASKNEWS_SECRET: ${{ secrets.ASKNEWS_SECRET }}
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        id: setup-python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install poetry (with fallback)
        id: install-poetry
        continue-on-error: true
        uses: snok/install-poetry@v1
        with:
          virtualenvs-create: true
          virtualenvs-in-project: true
          installer-parallel: true

      - name: Fallback to pip if poetry fails
        if: steps.install-poetry.outcome == 'failure'
        run: |
          python -m pip install --upgrade pip --timeout 60 --retries 3
          python -m pip install --timeout 60 --retries 3 poetry || true

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}

      - name: üîß Install dependencies with enhanced fallback
        id: install-deps
        run: |
          set -e

          echo "üîç Checking Python environment..."
          python --version
          pip --version

          echo "üì¶ Attempting Poetry dependencies installation..."
          # Try Poetry first but immediately fall back to pip if ANY issues
          if poetry install --only main 2>/dev/null && poetry run python --version >/dev/null 2>&1; then
            echo "‚úÖ Poetry installation successful"
            POETRY_SUCCESS=true
          else
            echo "‚ö†Ô∏è Poetry installation failed or environment broken, falling back to pip immediately..."
            POETRY_SUCCESS=false
          fi

          # Immediate pip fallback with essential packages
          if [ "$POETRY_SUCCESS" = "false" ]; then
            echo "üîÑ Installing essential packages via pip..."
            python -m pip install --upgrade pip

            # Install critical packages that are needed for the bot
            python -m pip install python-dotenv pydantic requests openai anthropic httpx aiofiles pyyaml typer pytest

            # Install forecasting_tools (critical for minibench bot)
            python -m pip install forecasting-tools || echo "‚ö†Ô∏è forecasting-tools not available via pip"

            # Try installing as editable package if possible
            python -m pip install -e . 2>/dev/null || echo "‚ö†Ô∏è Local package install failed"
          fi

          # Verify critical packages are available
          echo "üß™ Verifying critical dependencies..."
          python -c "
          import sys
          critical_packages = ['dotenv', 'pydantic', 'requests', 'openai']
          missing = []
          for pkg in critical_packages:
              try:
                  __import__(pkg)
                  print(f'‚úÖ {pkg}: available')
              except ImportError:
                  missing.append(pkg)
                  print(f'‚ùå {pkg}: missing')

          if missing:
              print(f'‚ö†Ô∏è Installing missing critical packages: {missing}')
              import subprocess
              for pkg in missing:
                  try:
                      subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])
                      print(f'‚úÖ Emergency install of {pkg} successful')
                  except:
                      print(f'‚ùå Emergency install of {pkg} failed')
          "


          echo "üìã Final package verification..."
          python -c "
          packages = ['dotenv', 'pydantic', 'requests', 'openai', 'forecasting_tools']
          for pkg in packages:
              try:
                  __import__(pkg)
                  print(f'‚úÖ {pkg}: imported successfully')
              except ImportError as e:
                  print(f'‚ùå {pkg}: {e}')
          "

      - name: üß™ Debug Python environment
        run: |
          echo "üîç Python Environment Debug:"
          python --version
          python -c "import sys; print('Python executable:', sys.executable)"
          python -c "import sys; print('Python path:', sys.path)"
          echo "üì¶ Installed packages:"
          pip list | head -20

          echo "üß™ Testing critical imports:"
          python -c "
          import sys
          try:
              import dotenv
              print('‚úÖ dotenv imported successfully')
          except ImportError as e:
              print(f'‚ùå Failed to import dotenv: {e}')
              sys.exit(1)
          "

      - name: Resolve MiniBench tournament ID
        id: resolve_id
        run: |
          SLUG="${{ github.event.inputs.tournament_slug || vars.AIB_MINIBENCH_TOURNAMENT_SLUG || '' }}"
          MID="${{ github.event.inputs.tournament_id || vars.AIB_MINIBENCH_TOURNAMENT_ID || '' }}"
          if [ -n "$SLUG" ]; then
            echo "‚úÖ Using MiniBench tournament slug: $SLUG"
            echo "should_run=true" >> "$GITHUB_OUTPUT"
            echo "minibench_target=$SLUG" >> "$GITHUB_OUTPUT"
          elif [ -n "$MID" ]; then
            echo "‚úÖ Using MiniBench tournament ID: $MID"
            echo "should_run=true" >> "$GITHUB_OUTPUT"
            echo "minibench_target=$MID" >> "$GITHUB_OUTPUT"
          else
            echo "‚ö†Ô∏è MiniBench slug/ID is not configured. Set repo var AIB_MINIBENCH_TOURNAMENT_SLUG='minibench' or AIB_MINIBENCH_TOURNAMENT_ID, or pass input.";
            echo "should_run=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Preflight validation (secrets and target)
        if: steps.resolve_id.outputs.should_run == 'true'
        run: |
          echo "üîé Preflight: validating secrets and target..."
          # Validate METACULUS_TOKEN (required for tournament fetching/submission)
          if [ -z "${METACULUS_TOKEN}" ]; then
            echo "‚ùå METACULUS_TOKEN is missing. Set repository secret METACULUS_TOKEN."
            printf '%s\n' \
              '{' \
              '  "run_mode": "tournament",' \
              '  "tournament_mode": "true",' \
              '  "publish_reports": "false",' \
              '  "successful_forecasts": 0,' \
              '  "failed_forecasts": 0,' \
              '  "total_processed": 0,' \
              '  "status": "failed",' \
              '  "reason": "Missing METACULUS_TOKEN secret"' \
              '}' > run_summary.json
            exit 1
          else
            echo "‚úÖ METACULUS_TOKEN is set"
          fi

          # Warn if OpenRouter key is missing (we can still run with fallbacks)
          if [ -z "${OPENROUTER_API_KEY}" ]; then
            echo "‚ö†Ô∏è OPENROUTER_API_KEY not set; will rely on proxy/fallbacks where possible"
          else
            echo "‚úÖ OPENROUTER_API_KEY is set"
          fi

          echo "üéØ Tournament target resolved: ${{ steps.resolve_id.outputs.minibench_target }}"
          echo "TOURNAMENT_MODE=true"
          echo "PUBLISH_REPORTS=true"

      - name: Write skipped summary (no tournament ID)
        if: steps.resolve_id.outputs.should_run != 'true'
        run: |
          printf '%s\n' \
            '{' \
            '  "run_mode": "tournament",' \
            '  "tournament_mode": "true",' \
            '  "tournament_id": "missing",' \
            '  "publish_reports": "false",' \
            '  "successful_forecasts": 0,' \
            '  "failed_forecasts": 0,' \
            '  "total_processed": 0,' \
            '  "status": "skipped",' \
            '  "reason": "AIB_MINIBENCH_TOURNAMENT_ID not configured and no input provided"' \
            '}' > run_summary.json
          echo "Wrote run_summary.json indicating skipped run."

      - name: Run MiniBench bot
        if: steps.resolve_id.outputs.should_run == 'true'
        id: run-bot
        run: |
          set -e

          # Robust Python runner detection with dependency verification
          echo "üîç Determining Python execution method..."
          if poetry env info >/dev/null 2>&1 && poetry run python --version >/dev/null 2>&1 && poetry run python -c "import pydantic, requests" 2>/dev/null; then
            echo "üöÄ Poetry environment verified and working - using Poetry runner"
            PY_RUN="poetry run python"
          else
            echo "üêç Poetry environment not available or broken - using system Python"
            PY_RUN="python"
            export PYTHONPATH="${PWD}/src:${PYTHONPATH:-}"

            # Emergency dependency check for system Python
            echo "üß™ Verifying system Python has critical dependencies..."
            python -c "
            import sys
            missing_critical = []
            for pkg in ['dotenv', 'pydantic', 'requests']:
                try:
                    __import__(pkg)
                except ImportError:
                    missing_critical.append(pkg)

            if missing_critical:
                print(f'‚ùå Critical packages missing: {missing_critical}')
                print('‚ö° Installing emergency dependencies...')
                import subprocess
                subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + missing_critical)
                print('‚úÖ Emergency dependencies installed')
            else:
                print('‚úÖ All critical dependencies available')
            " || echo "‚ö†Ô∏è Emergency dependency installation attempted"
          fi

          echo "üöÄ Running MiniBench bot for tournament ${{ steps.resolve_id.outputs.minibench_target }}"

          # Accept slug or ID in target envs
          if AIB_TOURNAMENT_SLUG=${{ steps.resolve_id.outputs.minibench_target }} \
             AIB_TOURNAMENT_ID=${{ steps.resolve_id.outputs.minibench_target }} \
             TOURNAMENT_MODE=true \
             PUBLISH_REPORTS=true \
             DRY_RUN=false \
             DISABLE_PUBLICATION_GUARD=true \
             SKIP_PREVIOUSLY_FORECASTED=true \
             $PY_RUN main.py --mode tournament; then
            echo "‚úÖ Bot execution completed successfully"
            echo "success=true" >> "$GITHUB_OUTPUT"
          else
            echo "‚ùå Bot execution failed"
            echo "success=false" >> "$GITHUB_OUTPUT"

            # Create fallback summary for failed executions (no heredoc for shellcheck compliance)
            ts="$(date -u +%Y-%m-%dT%H:%M:%SZ)"
            printf '%s\n' \
              '{' \
              '  "status": "failed",' \
              "  \"timestamp\": \"$ts\"," \
              '  "error": "Bot execution failed during MiniBench forecasting",' \
              '  "workflow_run": "${{ github.run_id }}",' \
              '  "commit": "${{ github.sha }}",' \
              '  "tournament_target": "${{ steps.resolve_id.outputs.minibench_target }}"' \
              '}' > run_summary.json

            exit 1
          fi
        env:
          LOG_LEVEL: INFO
          ENABLE_PROXY_CREDITS: true

      - name: Show run summary (if present)
        if: always()
        run: |
          if [ -f run_summary.json ]; then
            echo "üìÑ Run Summary:"
            cat run_summary.json
          else
            echo "‚ÑπÔ∏è run_summary.json not found."
          fi

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: minibench-run-${{ github.run_number }}
          path: |
            run_summary.json
            *.log
            logs/
          retention-days: 14
