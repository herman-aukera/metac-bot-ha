name: Forecast on MiniBench tournament

on:
  workflow_dispatch:
    inputs:
      tournament_slug:
        description: 'MiniBench tournament slug (e.g., minibench)'
        required: false
        type: string
      tournament_id:
        description: 'MiniBench tournament ID (overrides repo var)'
        required: false
        type: string
      scheduling_frequency_minutes:
        description: 'Run frequency in minutes'
        required: false
        default: '60'
        type: string
  schedule:
    # Check every 15 minutes by default (MiniBench can open short-lived questions)
    - cron: '*/15 * * * *'

concurrency:
  group: minibench-forecasting
  cancel-in-progress: true

jobs:
  minibench_forecast:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    env:
      METACULUS_TOKEN: ${{ secrets.METACULUS_TOKEN }}
      OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      ASKNEWS_CLIENT_ID: ${{ secrets.ASKNEWS_CLIENT_ID }}
      ASKNEWS_SECRET: ${{ secrets.ASKNEWS_SECRET }}
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        id: setup-python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install poetry (with fallback)
        id: install-poetry
        continue-on-error: true
        uses: snok/install-poetry@v1
        with:
          virtualenvs-create: true
          virtualenvs-in-project: true
          installer-parallel: true

      - name: Fallback to pip if poetry fails
        if: steps.install-poetry.outcome == 'failure'
        run: |
          python -m pip install --upgrade pip --timeout 60 --retries 3
          python -m pip install --timeout 60 --retries 3 poetry || true

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}

      - name: 🔧 Install dependencies with enhanced fallback
        id: install-deps
        run: |
          set -e

          echo "🔍 Checking Python environment..."
          python --version
          pip --version

          echo "📦 Installing Poetry dependencies..."
          if poetry install --no-dev; then
            echo "✅ Poetry installation successful"
            POETRY_SUCCESS=true
          else
            echo "⚠️ Poetry install failed, trying with verbose output..."
            poetry install --no-dev -vvv
            if [ $? -eq 0 ]; then
              echo "✅ Poetry installation successful with verbose mode"
              POETRY_SUCCESS=true
            else
              echo "❌ Poetry install failed completely"
              POETRY_SUCCESS=false
            fi
          fi

          # Test if virtual environment is working
          if [ "$POETRY_SUCCESS" = "true" ]; then
            echo "🧪 Testing Poetry environment..."
            if poetry run python -c "import sys; print(f'Python: {sys.executable}')"; then
              echo "✅ Poetry environment is working"
            else
              echo "⚠️ Poetry environment test failed"
              POETRY_SUCCESS=false
            fi
          fi

          # Enhanced fallback with essential packages
          if [ "$POETRY_SUCCESS" = "false" ]; then
            echo "🔄 Falling back to direct pip installation..."
            pip install --upgrade pip
            pip install python-dotenv pydantic requests openai anthropic PyYAML httpx aiohttp
            pip install -e .

            echo "🧪 Testing dotenv import..."
            python -c "import dotenv; print('✅ dotenv import successful')" || {
              echo "❌ dotenv import failed even after pip install"
              exit 1
            }
          fi

          echo "📋 Final package verification..."
          python -c "
          packages = ['dotenv', 'pydantic', 'requests', 'openai']
          for pkg in packages:
              try:
                  __import__(pkg)
                  print(f'✅ {pkg}: imported successfully')
              except ImportError as e:
                  print(f'❌ {pkg}: {e}')
          "

      - name: 🧪 Debug Python environment
        run: |
          echo "🔍 Python Environment Debug:"
          python --version
          python -c "import sys; print('Python executable:', sys.executable)"
          python -c "import sys; print('Python path:', sys.path)"
          echo "📦 Installed packages:"
          pip list | head -20

          echo "🧪 Testing critical imports:"
          python -c "
          import sys
          try:
              import dotenv
              print('✅ dotenv imported successfully')
          except ImportError as e:
              print(f'❌ Failed to import dotenv: {e}')
              sys.exit(1)
          "

      - name: Resolve MiniBench tournament ID
        id: resolve_id
        run: |
          SLUG="${{ github.event.inputs.tournament_slug || vars.AIB_MINIBENCH_TOURNAMENT_SLUG || '' }}"
          MID="${{ github.event.inputs.tournament_id || vars.AIB_MINIBENCH_TOURNAMENT_ID || '' }}"
          if [ -n "$SLUG" ]; then
            echo "✅ Using MiniBench tournament slug: $SLUG"
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "minibench_target=$SLUG" >> $GITHUB_OUTPUT
          elif [ -n "$MID" ]; then
            echo "✅ Using MiniBench tournament ID: $MID"
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "minibench_target=$MID" >> $GITHUB_OUTPUT
          else
            echo "⚠️ MiniBench slug/ID is not configured. Set repo var AIB_MINIBENCH_TOURNAMENT_SLUG='minibench' or AIB_MINIBENCH_TOURNAMENT_ID, or pass input.";
            echo "should_run=false" >> $GITHUB_OUTPUT
          fi

      - name: Write skipped summary (no tournament ID)
        if: steps.resolve_id.outputs.should_run != 'true'
        run: |
          cat > run_summary.json << 'JSON'
          {
            "run_mode": "tournament",
            "tournament_mode": "true",
            "tournament_id": "missing",
            "publish_reports": "false",
            "successful_forecasts": 0,
            "failed_forecasts": 0,
            "total_processed": 0,
            "status": "skipped",
            "reason": "AIB_MINIBENCH_TOURNAMENT_ID not configured and no input provided"
          }
          JSON
          echo "Wrote run_summary.json indicating skipped run."

      - name: Run MiniBench bot
        if: steps.resolve_id.outputs.should_run == 'true'
        id: run-bot
        run: |
          set -e

          # Determine how to run Python
          if command -v poetry >/dev/null 2>&1 && poetry env info >/dev/null 2>&1; then
            echo "🚀 Running via Poetry..."
            PY_RUN="poetry run python"
          else
            echo "🐍 Running via direct Python..."
            PY_RUN="python"
            export PYTHONPATH="${PWD}/src:${PYTHONPATH:-}"
          fi

          echo "🚀 Running MiniBench bot for tournament ${{ steps.resolve_id.outputs.minibench_target }}"

          # Accept slug or ID in target envs
          if AIB_TOURNAMENT_SLUG=${{ steps.resolve_id.outputs.minibench_target }} \
             AIB_TOURNAMENT_ID=${{ steps.resolve_id.outputs.minibench_target }} \
             TOURNAMENT_MODE=true \
             PUBLISH_REPORTS=true \
             DRY_RUN=false \
             $PY_RUN main.py --mode tournament; then
            echo "✅ Bot execution completed successfully"
            echo "success=true" >> "$GITHUB_OUTPUT"
          else
            echo "❌ Bot execution failed"
            echo "success=false" >> "$GITHUB_OUTPUT"

            # Create fallback summary for failed executions
            cat > run_summary.json << EOF
          {
            "status": "failed",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "error": "Bot execution failed during MiniBench forecasting",
            "workflow_run": "${{ github.run_id }}",
            "commit": "${{ github.sha }}",
            "tournament_target": "${{ steps.resolve_id.outputs.minibench_target }}"
          }
          EOF

            exit 1
          fi
        env:
          LOG_LEVEL: INFO
          ENABLE_PROXY_CREDITS: true

      - name: Show run summary (if present)
        if: always()
        run: |
          if [ -f run_summary.json ]; then
            echo "📄 Run Summary:"
            cat run_summary.json
          else
            echo "ℹ️ run_summary.json not found."
          fi

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: minibench-run-${{ github.run_number }}
          path: |
            run_summary.json
            *.log
            logs/
          retention-days: 14
